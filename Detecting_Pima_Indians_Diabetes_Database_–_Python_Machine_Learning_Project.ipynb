{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Detecting Pima Indians Diabetes Database – Python Machine Learning Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCudNOpwVwnR"
      },
      "source": [
        "> Welcome to the project! You will find tips in quoted sections like this to help organize your approach to your investigation.\n",
        "\n",
        "# Project: Investigate a Pima Indians Diabetes Dataset\n",
        "\n",
        "## Table of Contents\n",
        "<ul>\n",
        "<li><a href=\"#intro\">Introduction</a></li>\n",
        "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
        "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
        "<li><a href=\"#pre\">Prediction analysis</a></li>\n",
        "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iEZF9gjVwnS"
      },
      "source": [
        "<a id='intro'></a>\n",
        "## Introduction\n",
        "\n",
        "> Diabetes is one of the deadliest diseases in the world. It is not only a disease but also creator of different kinds of diseases like heart attack, blindness etc. The normal identifying process is that patients need to visit a diagnostic center, consult their doctor, and sit tight for a day or more to get their reports. So, the objective of this project is to identify whether the patient has diabetes or not based on diagnostic measurements.. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
        ">\n",
        "> The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
        "\n",
        "> About Dataset:\n",
        "\n",
        ">> Pregnancies: No. of times pregnant\n",
        "\n",
        ">> Glucose: Plasma Glucose Concentration (mg/dl)\n",
        "\n",
        ">> Blood Pressure: Diastolic Blood Pressure(mmHg)\n",
        "\n",
        ">> Skin Thickness:A value used to estimate body fat. Normal Triceps SkinFold Thickness in women is 23mm. Higher thickness leads to obesity and chances of diabetes increases.\n",
        "\n",
        ">> Insulin: 2-Hour Serum Insulin (mu U/ml)\n",
        "\n",
        ">> BMI: Body Mass Index (weight in kg/ height in m2)\n",
        "\n",
        ">> Diabetes Pedigree Function: It provides information about diabetes history in relatives and genetic relationship of those relatives with patients. Higher Pedigree Function means patient is more likely to have diabetes.\n",
        "\n",
        ">> Age:Age (years)\n",
        "\n",
        ">> Outcome: Class Variable (0 or 1) where ‘0’ denotes patient is not having diabetes and ‘1’ denotes patient having diabetes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRfP3eYyVwnT"
      },
      "source": [
        "# Use this cell to set up import statements for all of the packages that you plan to use.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "#sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6eoZ7MDPXQW"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(12,10)})\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evfwmhQIVwnU"
      },
      "source": [
        "<a id='wrangling'></a>\n",
        "## Data Wrangling\n",
        "\n",
        "> **Tip**: In this section of the report, you will load in the data, check for cleanliness, and then trim and clean your dataset for analysis. Make sure that you document your steps carefully and justify your cleaning decisions.\n",
        "\n",
        "### General Properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RNFZf5lVVwnU"
      },
      "source": [
        "# Load your data and print out a few lines. Perform operations to inspect data\n",
        "#   types and look for instances of missing or possibly errant data.\n",
        "file_name=\"diabetes.csv\"\n",
        "df=pd.read_csv(file_name)\n",
        "print(df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZWWKSt01NOd"
      },
      "source": [
        "#Print a few samples from head\n",
        "display(df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK-fRZ74x0HO"
      },
      "source": [
        "#Check if there are any null values\n",
        "display(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41KHAotx4FAE"
      },
      "source": [
        "#Check if there are any symbols like ? or strings in the dataframe\n",
        "df.applymap(np.isreal).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUyRGS4Q1JYW"
      },
      "source": [
        "#Basic statistics for each column\n",
        "display(df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN0Cawv03xAS"
      },
      "source": [
        "We notice in the table above abnormal range of values for things like glucose, blood pressure and skin thickness and BMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrQ9ub8I1fuq"
      },
      "source": [
        "#print some value counts for discrete columns\n",
        "print(df['Pregnancies'].value_counts())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awRu9D8T4h7g"
      },
      "source": [
        "#Check class imbalance\n",
        "ar = df['Outcome'].value_counts()\n",
        "print(df['Outcome'].value_counts())\n",
        "print('Class Doesnt have diabetes :', ar[0] / (ar[0]+ar[1]),'%' ) \n",
        "print('Class Have diabetes :', ar[1] / (ar[0]+ar[1] ),'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jwaLr-K9FkZ"
      },
      "source": [
        "plt.pie(df['Outcome'].value_counts(),labels=[\"Doesnt have diabetes\",\"Have diabetes\"],autopct='%1.1f%%' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTW8czhk_H6A"
      },
      "source": [
        "#Plot Histogram for all columns\n",
        "f,a = plt.subplots(2,4)\n",
        "a = a.ravel()\n",
        "for idx,ax in enumerate(a):\n",
        "  try:\n",
        "    ax.hist((df[df.columns[idx]].astype(np.float16) ))\n",
        "    ax.set_title(df.columns[idx])\n",
        "  except :\n",
        "    pass\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gfwjCxF9zf5"
      },
      "source": [
        "#Plot The Disease by each age group\n",
        "((df['Age'] [(df['Outcome']==0)] )).plot.hist(bins=20,alpha=0.3)\n",
        "((df['Age'] [(df['Outcome']==1)] )).plot.hist(bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-yImDb-O5I"
      },
      "source": [
        "#Quartile Plot for Age\n",
        "ax = sns.boxplot(y=df['Age'],x=df['Outcome'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbL-mxqr-Z_6"
      },
      "source": [
        "#Plot The Disease by each number of pregnancies\n",
        "((df['Pregnancies'] [(df['Outcome']==0)] )).plot.hist(bins=20,alpha=0.3)\n",
        "((df['Pregnancies'] [(df['Outcome']==1)] )).plot.hist(bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrDifk7AVwnV"
      },
      "source": [
        "> **Tip**: Make sure that you keep your reader informed on the steps that you are taking in your investigation. Follow every code cell, or every set of related code cells, with a markdown cell to describe to the reader what was found in the preceding cell(s). Try to make it so that the reader can then understand what they will be seeing in the following cell(s).\n",
        "\n",
        "### Data Cleaning by replacing the missing values using interpolation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MnH3fS1VwnV"
      },
      "source": [
        "# After discussing the structure of the data and any problems that need to be\n",
        "#   cleaned, perform those cleaning steps in the second part of this section.\n",
        "# BMI, glucose,SkinThickness, Bloodpressure\n",
        "\n",
        "#check where the zero values lie\n",
        "print((df==0).sum())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCFRXSy06qt4"
      },
      "source": [
        "\n",
        "for col in ['BMI','Glucose', 'SkinThickness','BloodPressure']:\n",
        "  print(col)\n",
        "  af=(df[col]==0)\n",
        "  df[col][af]=np.nan\n",
        "  print(df[col][af])\n",
        "  df[col]=df[col].interpolate()\n",
        "  print(df[col][af])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgOHgtpaVwnV"
      },
      "source": [
        "<a id='eda'></a>\n",
        "## Exploratory Data Analysis\n",
        "\n",
        "> **Tip**: Now that you've trimmed and cleaned your data, you're ready to move on to exploration. Compute statistics and create visualizations with the goal of addressing the research questions that you posed in the Introduction section. It is recommended that you be systematic with your approach. Look at one variable at a time, and then follow it up by looking at relationships between variables.\n",
        "\n",
        "### Research Question 1 : Data Correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVUiA6qC8m-o"
      },
      "source": [
        "Let's begin by observing correlation of the columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C27iZO9l8rtd"
      },
      "source": [
        "display(df.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OkPgJMw9kvS"
      },
      "source": [
        "#Visualized\n",
        "ax = sns.heatmap(df.corr(),annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ceai0goV8vHL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyqEKyAUHNfP"
      },
      "source": [
        "### Research Question 2 : Class Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lRkipWBHQVA"
      },
      "source": [
        "How do we handle the class imbalance and prevent model from overfitting and predicting execlusively one class only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp2UOL-eHjlh"
      },
      "source": [
        "There are two methods over sampling and under sampling \n",
        "we will try over sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi7xvOdlHWMB"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df.Outcome==0]\n",
        "df_minority = df[df.Outcome==1]\n",
        " \n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority, \n",
        "                                 replace=True,     \n",
        "                                 n_samples=500,    \n",
        "                                 random_state=1) \n",
        " \n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "#print((df_upsampled.iloc[:,-1].values.shape) )\n",
        "df_upsampled.Outcome.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j5GL9T9IZ6o"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(df_upsampled.iloc[:,:-1], df_upsampled.iloc[:,-1].values.reshape(-1,1), test_size=0.2, random_state=1,shuffle= True)\n",
        "print(Y_test .shape, X_test.shape)\n",
        "print(Y_train .shape, X_train.shape)\n",
        "\n",
        "print(Y_train.sum() /len(X_train ))\n",
        "print(Y_test.sum()/len(X_test ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlUew2aXHWSz"
      },
      "source": [
        "\n",
        "model = LinearRegression()\n",
        "scores = []\n",
        "epochs=1\n",
        "for i in range(epochs): \n",
        "  model.fit(X_train, Y_train )\n",
        "  score = model.score(X_test, Y_test)\n",
        "  scores.append(score)\n",
        "predictions=model.predict(X_test)\n",
        "print(scores)\n",
        "eval=(1*(predictions>0.5)==Y_test)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print(eval, \"%\")\n",
        "print(classification_report(Y_test,1*(predictions>0.5)))\n",
        "\n",
        "print(confusion_matrix(Y_test,1*(predictions>0.5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrAS6tHEF6On"
      },
      "source": [
        "#**PLEASE FIND RESEARCH QUESTION 3 AFTER CLASSIFIERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-rN6IuaVwnX"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6IUyYu3VwnX"
      },
      "source": [
        "> **Note: if you have more questions and insights don't hesitate to do it**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnLVQ6MeVwnX"
      },
      "source": [
        "<a id='pre'></a>\n",
        "## Build a Prediction Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tqyvr1LVwnY"
      },
      "source": [
        "# Make a feature scaling\n",
        "#Normalize Values\n",
        "normalized_df=(df-df.min())/(df.max()-df.min())\n",
        "#print(normalized_df)\n",
        "display(normalized_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC-CX9v0Df-4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh7nTx3LA-vq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlGVWK4PVwnY"
      },
      "source": [
        "# Split the data into train and test data\n",
        "\n",
        "msk = np.random.rand(len(normalized_df)) < 0.8\n",
        "normalized_df= normalized_df.sample(frac = 1,random_state=1)\n",
        "train = normalized_df[msk]\n",
        "test = normalized_df[~msk]\n",
        "\n",
        "X_train=train.iloc[:, 0:-1].values\n",
        "Y_train=train.iloc[:, -1].values.reshape(-1,1)\n",
        "X_test=test.iloc[:, 0:-1].values\n",
        "Y_test=test.iloc[:,-1].values.reshape(-1,1)\n",
        "\n",
        "#print(X_train.shape,Y_train.shape, X_test.shape, Y_test.shape)\n",
        "print(Y_test .shape, X_test.shape)\n",
        "print(Y_train .shape, X_train.shape)\n",
        "\n",
        "print(Y_train.sum() /len(X_train ))\n",
        "print(Y_test.sum()/len(X_test ))\n",
        "\n",
        "#print(test.info(), train.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z8ZEKbOVwnY"
      },
      "source": [
        "## Compare the performance (Confusion matrix and classification report) of different classifiers (LR, KNN, SVM, DT and RF)\n",
        "\n",
        "> **Note: use grid search with a suitable range of values to adjust the hyperparameters of DT and SVM and for loop to adjust the k value of KNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SOl77WVVwnZ"
      },
      "source": [
        "model = LinearRegression()\n",
        "scores = []\n",
        "epochs=1\n",
        "for i in range(epochs): \n",
        "  model.fit(X_train, Y_train )\n",
        "  score = model.score(X_test, Y_test)\n",
        "  scores.append(score)\n",
        "predictions=model.predict(X_test)\n",
        "print(scores)\n",
        "eval=(1*(predictions>0.5)==Y_test)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print('Accuracy : ',eval, \"%\")\n",
        "print('classification_report\\n',classification_report(Y_test,1*(predictions>0.5)))\n",
        "\n",
        "print('confusion_matrix\\n',confusion_matrix(Y_test,1*(predictions>0.5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yamZT2ppnH6F"
      },
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_train, Y_train)\n",
        "\n",
        "predict=neigh.predict(X_test).reshape(-1,1)\n",
        "\n",
        "eval=(predict==Y_test)\n",
        "print(predict.shape, Y_test.shape)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print('Accuracy : ',eval, \"%\")\n",
        "print('classification_report\\n',classification_report(Y_test,predict))\n",
        "print('confusion_matrix\\n',confusion_matrix(Y_test,predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhMfKYwnpgGP"
      },
      "source": [
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf.fit(X_train, Y_train)\n",
        "predict=clf.predict(X_test).reshape(-1,1)\n",
        "eval=(predict==Y_test)\n",
        "print(predict.shape, Y_test.shape)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print('Accuracy : ',eval, \"%\")\n",
        "print('classification_report\\n',classification_report(Y_test,predict))\n",
        "print('confusion_matrix\\n',confusion_matrix(Y_test,predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ifBDPp5qUW8"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train, Y_train)\n",
        "predict=clf.predict(X_test).reshape(-1,1)\n",
        "eval=(predict==Y_test)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print('Accuracy : ',eval, \"%\")\n",
        "print('classification_report\\n',classification_report(Y_test,predict))\n",
        "print('confusion_matrix\\n',confusion_matrix(Y_test,predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5dpvKYKrjRu"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "clf = MLPClassifier( alpha=1e-3,\n",
        "                    hidden_layer_sizes=(10, 20), random_state=1)\n",
        "for i in range(5):\n",
        "  clf.fit(X_train, Y_train)\n",
        "predict=clf.predict(X_test).reshape(-1,1)\n",
        "eval=(predict==Y_test)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print('Accuracy : ',eval, \"%\")\n",
        "print('classification_report\\n',classification_report(Y_test,predict))\n",
        "print('confusion_matrix\\n',confusion_matrix(Y_test,predict))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rha4LMNtnvK"
      },
      "source": [
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
        "clf.fit(X_test, Y_test)\n",
        "predict=clf.predict(X_test).reshape(-1,1)\n",
        "eval=(predict==Y_test)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print('Accuracy : ',eval, \"%\")\n",
        "print('classification_report\\n',classification_report(Y_test,predict))\n",
        "print('confusion_matrix\\n',confusion_matrix(Y_test,predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqv8_W0iVwnZ"
      },
      "source": [
        "<a id='conclusions'></a>\n",
        "## Conclusions\n",
        "\n",
        "> **Tip**: Finally, summarize your findings and the results that have been performed. Make sure that you are clear with regards to the limitations of your exploration. If you haven't done any statistical tests, do not imply any statistical conclusions. And make sure you avoid implying causation from correlation!\n",
        "\n",
        "> **Tip**: Once you are satisfied with your work send it and Congratulations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnG0T7OIVwnX"
      },
      "source": [
        "## Research Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MORwdRNPHEN0"
      },
      "source": [
        "PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97L1DogDGih4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YpkAPmPGiln"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "df_pca = pca.fit_transform(normalized_df.iloc[:,:-1])\n",
        "df_pca = pd.DataFrame(df_pca)\n",
        "#print(normalized_df.iloc[:,-1])\n",
        "df_pca['Outcome'] =  normalized_df.iloc[:,-1]\n",
        "print(df_pca.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK9xmXi8GisX"
      },
      "source": [
        "# Split the data into train and test data\n",
        "df_pca= df_pca.sample(frac = 1)\n",
        "msk = np.random.rand(len(df_pca)) < 0.8\n",
        "train = df_pca[msk]\n",
        "test = df_pca[~msk]\n",
        "\n",
        "X_train=train.iloc[:, 0:-1].values\n",
        "Y_train=train.iloc[:, -1].values.reshape(-1,1)\n",
        "X_test=test.iloc[:, 0:-1].values\n",
        "Y_test=test.iloc[:,-1].values.reshape(-1,1)\n",
        "\n",
        "#print(X_train.shape,Y_train.shape, X_test.shape, Y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcBaZ8ArOR2X"
      },
      "source": [
        "model = LinearRegression()\n",
        "scores = []\n",
        "epochs=1\n",
        "for i in range(epochs): \n",
        "  model.fit(X_train, Y_train )\n",
        "  score = model.score(X_test, Y_test)\n",
        "  scores.append(score)\n",
        "predictions=model.predict(X_test)\n",
        "print(scores)\n",
        "eval=(1*(predictions>0.5)==Y_test)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print('Accuracy : ',eval, \"%\")\n",
        "print('classification_report\\n',classification_report(Y_test,1*(predictions>0.5)))\n",
        "\n",
        "print('confusion_matrix\\n',confusion_matrix(Y_test,1*(predictions>0.5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUoqoYr3O_CH"
      },
      "source": [
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
        "clf.fit(X_test, Y_test)\n",
        "predict=clf.predict(X_test).reshape(-1,1)\n",
        "eval=(predict==Y_test)\n",
        "eval=eval.sum()\n",
        "eval/=len(Y_test)\n",
        "eval*=100\n",
        "print('Accuracy : ',eval, \"%\")\n",
        "print('classification_report\\n',classification_report(Y_test,predict))\n",
        "print('confusion_matrix\\n',confusion_matrix(Y_test,predict))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}